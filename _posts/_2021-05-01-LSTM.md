---
title: "LSTM"
excerpt: "RNN + GATE = ?"

categories:
    NLP
tags:
    파이썬
    RNN
    밑바닥 부터 시작하는 딥러닝2
toc: true
comments : true
---

# 개요  
RNN은 구조가 단순해서 구현하기 쉽지만 치명적인 단점이 있는데 BPTT(Back Propagation Through Time)에서  <span style ="color: orange">Gradient Vanishing</span>, 혹은 <span style ="color: orange">Gradient Exploding</span>이 일어나기 때문에 시계열 데이터와 같이 시간적으로 멀리 떨어지는 <span style = "color : orange"> 장기 의존 관계를 잘 학습할 수 없다는 것</span>이다.  

<span style = "font-size : 20px">한마디로 정작 중요한 <span style ="color: orange">성능이 좋지 못하다는 것</span>이다.  </span>


이를 해결하기 위해서 기존 RNN 구조에 <span style ="color: orange">게이트구조</span>가 추가 되는데 대표적으로 <span style ="color: orange">LSTM</span>, <span style ="color: orange">GRU</span>이 있다.  

이번 포스팅에서는 __LSTM__ 에 대한 정리를 한다.  

-------------------------------------

# 소실과 폭발에 대한 대책  
Gradient Vanishing, Exploding은 행렬에 곱해지는 <span style = "color : orange">동일한 값의 가중치</span> 때문에 일어난다.(Vanishing이냐 Exploding이냐는 입력에 의해 구해지는 행렬이 1에 비해 큰지 크지 않은지에 달려있다.)  

이 장기 의존 관계에서의 Vanishing과 Exploding을 해결하는 것이 결국 중요한 과제가 되는데,  
그 중 에서 폭발에 대한 전통적인 __Gradient Clipping__ 이라는 기법이 있다.   

## Gradient Clipping  

클리핑의 알고리즘을 의사코드로 표현하면 아래와 같다.  

<p align = "center"><img src = "../../assets/images/LSTM/Clipping_seudo.jpg"></p>  

<p align = "center"><span style = "font-size : 12px">||g^||는 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것, threshold는 문턱 값을 뜻한다.</span>
</p>  
  

의사 코드를 통해서 기울기의 L2 norm이 문턱값 이상이면 그 아래의 수식과 같이 기울기를 수정(문턱 값을 전체 기울기 값(\|\|g^\|\|)으로 나눈 뒤 각 기울기에 곱하는 방식)한다는 의미임을 알 수 있다. 간단한 알고리즘이지만 대체로 잘 작동한다고 한다.  

Exploding에 대한 기법은 Clipping으로 해결 가능하다! (대체로는..)  

## LSTM
하지만 또 하나의 큰 과제인 Vanishing에 대한 대책도 필요하다. 이를 해결하기 위해서는 RNN아키텍쳐를 해부하는 수준 까지 가야하는데, 이 때 __LSTM__ 과 __GRU__ 가 등장한다. 물론 LSTM에 대한 포스팅이기 때문에 LSTM에 대해서만 언급한다.  
-----------------------------6.2 LSTM 기울기 소실부터 작성하자---------------------------











